PROMPT FOR AGENT 3 — “Opus Orb → Voice Mode (OpenAI Realtime)”

Goal
When the user clicks the Opus Orb, toggle “Voice Mode”: capture mic audio, stream it to OpenAI’s Realtime model, and play the model’s audio replies. Click again to end the session.

Why this approach

Uses OpenAI Realtime API over WebRTC for true low-latency speech-in/speech-out. 
OpenAI Platform
+2
OpenAI Platform
+2

Uses standard getUserMedia for mic capture and streams via WebRTC; the model sends back an audio track we can attach to an <audio> element. 
MDN Web Docs
+1

1) Backend: issue ephemeral Realtime tokens

Add a minimal server route that mints an ephemeral Realtime session token so the browser never sees our real API key.

Route: POST /api/openai/realtime/token

// server/routes/openaiRealtime.ts
import express from "express";
import fetch from "node-fetch"; // or global fetch on Node 18+
const router = express.Router();

const OPENAI_API_KEY = process.env.OPENAI_API_KEY!;
const REALTIME_MODEL = process.env.OPENAI_REALTIME_MODEL || "gpt-4o-realtime-preview-2024-12-17";

router.post("/token", async (req, res) => {
  try {
    // Create an ephemeral session (valid ~1 min). Model must be a Realtime-capable model.
    const r = await fetch("https://api.openai.com/v1/realtime/sessions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${OPENAI_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: REALTIME_MODEL,
        // Optional: set a voice, instructions, or tools here
        // voice: "verse", modalities: ["audio","text"]
      }),
    });

    if (!r.ok) {
      const txt = await r.text();
      return res.status(500).json({ error: "Failed to create session", details: txt });
    }
    const json = await r.json();
    return res.json(json); // includes client_secret.value we’ll use as ephemeral token
  } catch (e:any) {
    return res.status(500).json({ error: e?.message || "Unknown error" });
  }
});

export default router;


Wire it up in Express:

// server/index.ts
import openaiRealtime from "./routes/openaiRealtime";
// ...
app.use("/api/openai/realtime", openaiRealtime);


Env vars (backend)

OPENAI_API_KEY=sk-...
OPENAI_REALTIME_MODEL=gpt-4o-realtime-preview-2024-12-17


Refs: OpenAI Realtime sessions & WebRTC setup. 
OpenAI Platform
+1

2) Frontend helper: start/stop a WebRTC session

Create a small client util that:

fetches the ephemeral token

creates a RTCPeerConnection

adds the user’s mic track

negotiates with OpenAI’s Realtime endpoint

plays the remote audio in an <audio> element

// src/lib/voice/realtimeClient.ts
export type RealtimeHandle = {
  stop: () => void;
  isActive: () => boolean;
};

export async function startRealtimeVoice(audioEl: HTMLAudioElement): Promise<RealtimeHandle> {
  let pc: RTCPeerConnection | null = null;
  let localStream: MediaStream | null = null;

  // 1) Get ephemeral token for WebRTC
  const sess = await fetch("/api/openai/realtime/token", { method: "POST" }).then(r => r.json());
  const EPHEMERAL_KEY = sess?.client_secret?.value;
  if (!EPHEMERAL_KEY) throw new Error("No ephemeral key from server");

  // 2) Create RTCPeerConnection
  pc = new RTCPeerConnection();

  // 3) Play remote audio
  pc.ontrack = (e) => {
    // OpenAI sends an audio track; attach it to our <audio>
    if (audioEl.srcObject !== e.streams[0]) {
      audioEl.srcObject = e.streams[0];
      audioEl.play().catch(() => {});
    }
  };

  // 4) Capture mic (audio only). Must be triggered by a user gesture.
  localStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
  for (const track of localStream.getTracks()) {
    pc.addTrack(track, localStream);
  }

  // 5) Data channel (optional: send text, barge-in, UI signals)
  pc.createDataChannel("oai-events");

  // 6) Create an SDP offer
  const offer = await pc.createOffer({ offerToReceiveAudio: true, offerToReceiveVideo: false });
  await pc.setLocalDescription(offer);

  // 7) Send offer to OpenAI’s Realtime WebRTC endpoint
  const baseUrl = "https://api.openai.com/v1/realtime";
  const model = "gpt-4o-realtime-preview-2024-12-17";
  const r = await fetch(`${baseUrl}?model=${encodeURIComponent(model)}`, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${EPHEMERAL_KEY}`,
      "Content-Type": "application/sdp",
    },
    body: offer.sdp,
  });

  if (!r.ok) {
    throw new Error(`OpenAI Realtime handshake failed: ${await r.text()}`);
  }

  // 8) Apply remote SDP answer
  const answer = { type: "answer" as const, sdp: await r.text() };
  await pc.setRemoteDescription(answer);

  const stop = () => {
    try {
      pc?.getSenders().forEach(s => s.track?.stop());
      localStream?.getTracks().forEach(t => t.stop());
      pc?.close();
    } catch {}
    pc = null;
    localStream = null;
    if (audioEl) {
      audioEl.pause();
      audioEl.srcObject = null;
    }
  };

  const isActive = () => !!pc;

  return { stop, isActive };
}


Refs: WebRTC mic capture & getUserMedia usage. 
MDN Web Docs
+1

Refs: OpenAI Realtime conversation & WebRTC handshake. 
OpenAI Platform
+1

3) Hook it to the Opus Orb

Add a hidden audio element and a small controller to the Orb component. On first click: start. On second click: stop. Show a glowing “Listening” state.

// src/features/orb/OpusOrb.tsx
import { useRef, useState } from "react";
import { startRealtimeVoice } from "@/lib/voice/realtimeClient";

export default function OpusOrb() {
  const audioRef = useRef<HTMLAudioElement>(null);
  const handleRef = useRef<ReturnType<typeof startRealtimeVoice> | null>(null);
  const [listening, setListening] = useState(false);
  const [busy, setBusy] = useState(false);

  const toggleVoice = async () => {
    if (busy) return;
    setBusy(true);
    try {
      if (!listening) {
        // Start voice
        if (!audioRef.current) return;
        handleRef.current = await startRealtimeVoice(audioRef.current);
        setListening(true);
      } else {
        // Stop voice
        handleRef.current?.stop();
        handleRef.current = null;
        setListening(false);
      }
    } catch (e) {
      console.error("Voice toggle error:", e);
      // Optional: toast error
    } finally {
      setBusy(false);
    }
  };

  return (
    <div className="flex flex-col items-center gap-3">
      <button
        onClick={toggleVoice}
        aria-pressed={listening}
        className={[
          "h-28 w-28 rounded-full border relative transition-shadow",
          "border-zinc-700 bg-black/40 backdrop-blur",
          listening
            ? "shadow-[0_0_30px_5px_rgba(59,130,246,0.65)]"
            : "shadow-[0_0_16px_2px_rgba(255,255,255,0.15)] hover:shadow-[0_0_22px_3px_rgba(255,255,255,0.25)]",
        ].join(" ")}
        title={listening ? "Stop Voice" : "Start Voice"}
      >
        <div className="absolute inset-0 rounded-full" />
        <div className="absolute inset-3 rounded-full border border-zinc-700" />
        {/* small pulsing dot when active */}
        {listening && (
          <span className="absolute -right-1 -top-1 h-3 w-3 rounded-full bg-white animate-pulse" />
        )}
      </button>

      <div className="text-xs text-zinc-400">
        {listening ? "Listening… (tap to end)" : "Tap to speak with Opus"}
      </div>

      {/* Hidden audio sink for remote voice */}
      <audio ref={audioRef} autoPlay playsInline hidden />
    </div>
  );
}

4) UX & permissions

The first click must be a user gesture to grant microphone permission via getUserMedia. If permission is denied, show a toast explaining how to enable the mic. 
MDN Web Docs

Run over HTTPS in production; browsers require secure context for mic/WebRTC.

5) (Optional) Model behavior

You can set defaults (voice, system instructions, tools) when creating the session on the server (see body of /v1/realtime/sessions). For a voice selection or text+voice modes, see OpenAI audio/voice guides. 
OpenAI Platform
+1

6) Acceptance criteria

Clicking the Orb starts/stops a live voice session.

You can speak; the assistant replies out loud in < 1–2 seconds typical round-trip.

Stopping the session releases the mic and removes the remote audio track.

No API key ever appears in network tab (only short-lived ephemeral token). 
OpenAI Platform

7) Notes / gotchas

Ephemeral tokens expire quickly; mint a fresh token each start.

Handle tab visibility changes—stop session on logout or route change to avoid “ghost mic” capture.

If we later want text overlay of what the model is saying or function calls, open a data channel and parse Realtime events (see Realtime conversations guide). 
OpenAI Platform